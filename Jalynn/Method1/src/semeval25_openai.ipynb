{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7c6c379-14f0-4423-9f19-5a2d7efee937"
   },
   "source": [
    "# Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1189959c-2faa-4b3f-8f5b-b8c41cceaf31",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.338141Z",
     "start_time": "2025-01-17T10:34:47.327087Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import httpx\n",
    "from RefChecker.refchecker.extractor import extractor_prompts\n",
    "# import RefChecker\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scorer import recompute_hard_labels\n",
    "import glob\n",
    "import re\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "40d80ff5-4707-404f-ae56-e61a5dbab507",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.374115Z",
     "start_time": "2025-01-17T10:34:47.363319Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\")\n"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFOs4S7BY7ui"
   },
   "source": [
    "## 1. Extracting Claims (Extractor) - Each claim is a merger of triple-structured knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f47d942e-0617-40e0-b1a5-1c28f4f6a7d7",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.379648Z",
     "start_time": "2025-01-17T10:34:47.377700Z"
    }
   },
   "source": [
    "LLM_TRIPLET_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_TRIPLET_EXTRACTION_PROMPT_Q\n",
    "LLM_Triplet_To_Claim_PROMPT_Q = extractor_prompts.LLM_Triplet_To_Claim_PROMPT_Q\n",
    "LLM_CLAIM_EXTRACTION_PROMPT_Q = extractor_prompts.LLM_CLAIM_EXTRACTION_PROMPT_Q"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d59a72f6-e73d-482d-bc9c-e263ef495d27",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.388118Z",
     "start_time": "2025-01-17T10:34:47.385710Z"
    }
   },
   "source": [
    "def extract_triplets_to_claims(question, model_output_text):\n",
    "    prompt = LLM_CLAIM_EXTRACTION_PROMPT_Q.format(q=question, r=model_output_text)\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant who extracts claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e78c8d47-15c9-4c68-a4b0-ec200591c5fc"
   },
   "source": [
    "## 2. Obtain Complete References"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1a0a7428-4377-4cbb-9a53-6a0df26d48c0",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.397582Z",
     "start_time": "2025-01-17T10:34:47.394728Z"
    }
   },
   "source": [
    "def get_reference_for_claim(claim):\n",
    "    prompt = f\"\"\"\n",
    "    Please expand, provide additional relevant factual information and verify the following claim:\n",
    "    Claims: {claim}\n",
    "\n",
    "    If the claim is accurate, return the original claim.\n",
    "    If the claim is inaccurate or incomplete, return a corrected, more detailed statement.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant verifying claims.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(f\"No response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "        if not response_content.strip():\n",
    "            print(f\"No content in the response for the prompt: {prompt}\")\n",
    "            return []\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return []\n"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9735211d-f550-4a22-b52c-fa523d0faf5c",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.404307Z",
     "start_time": "2025-01-17T10:34:47.402355Z"
    }
   },
   "source": [
    "def extract_and_get_references(claims, context):\n",
    "    references = []\n",
    "    for claim in claims:\n",
    "        verified_reference = get_reference_for_claim(claim)\n",
    "        references.append(verified_reference)\n",
    "\n",
    "    final_reference = \" \".join(references) + \" \" + context\n",
    "\n",
    "    return final_reference"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de5f64f8-c12d-4bd9-a90e-f5c1ab591941"
   },
   "source": [
    "## 3. Validate claims, `model_input`, `model_output_text`, and References (Checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a03c1421-e879-41c1-b162-eb72fcc801d0"
   },
   "source": [
    "The validation results should be mapped back to the `model_output_text`, marking hallucination positions and probabilities, and outputting them as `soft_labels`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "76c4340a-731e-4048-8449-0e53fa9b0f24",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.411657Z",
     "start_time": "2025-01-17T10:34:47.409098Z"
    }
   },
   "source": [
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    # parse JSON data\n",
    "    try:\n",
    "        hallucination_results = json.loads(hallucination_results)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to decode JSON. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}\n"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe707e17-1b23-4cd7-9955-263678640b67",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.419294Z",
     "start_time": "2025-01-17T10:34:47.416170Z"
    }
   },
   "source": [
    "def triplets_and_references_checker(claims, model_output_text, references, question):\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the model output text for hallucinations by comparing it to the provided references, existed fact, claims, and question (model input). Identify any hallucinated or potentially inaccurate parts in the entire model output text. Highlight the hallucinated word and assign a probability of the hallucination word in the `model output text`.\n",
    "\n",
    "    ### Question (Model Input)\n",
    "    {question}\n",
    "\n",
    "    ### Claims\n",
    "    {claims}\n",
    "\n",
    "    ### References\n",
    "    {references}\n",
    "\n",
    "    ### Model Output Text\n",
    "    {model_output_text}\n",
    "\n",
    "    ### Instructions\n",
    "    1. Compare each claim with the provided references, question and existing fact (internal knowledge).\n",
    "    2. If a claim cannot be fully supported by the references, identify the hallucinated words and mark it to `model output text`.\n",
    "    3. Return character-level offsetss and assign hallucination probabilities.\n",
    "    4. If the claim is fully supported, hallucination should not to be labeled.\n",
    "    5. Assign hallucination probabilities based on the following criteria:\n",
    "       - **0.7 - 1.0**: Fully fabricated or highly speculative content with no supporting evidence.\n",
    "       - **0.4 - 0.7**: Partially incorrect or speculative content, but some evidence supports parts of the claim.\n",
    "       - **0.1 - 0.4**: Minor inaccuracies, such as spelling errors, wrong formatting, or small factual deviations.\n",
    "    6. Ensure that the hallucinated words do not overlap or repeat. If overlapping occurs, merge them or seperate them appropriately.\n",
    "    7. Ensure the words are shown in the `model output text`.\n",
    "    8. Highlight text in `model output text` that could potentially be a hallucination even if not explicitly listed in the claims.\n",
    "    9. Return **all the hallucinated words or phrases** and assign each a hallucination probability (between 0 and 1).\n",
    "    10. Do not filter out hallucinations based on low probability. Return results for any potential hallucination.\n",
    "    11. Do not include any explanations, summaries, or additional text. **Return the JSON list directly.**\n",
    "    12. Ensure all potential hallucinations are listed, even those with probabilities as low as 0.1.\n",
    "\n",
    "    ### Output Example\n",
    "    Only return results with all hallucinated words or phrases and their probability **strictly in the following JSON format**:\n",
    "    [\n",
    "        {{\"word\": <example_word>, \"prob\": <probability>}},\n",
    "        {{\"word\": <another_word>, \"prob\": <probability>}}\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"You are an AI assistant who checks the factual accuracy of claims and returns position and probability of the hallucination from model output text\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "\n",
    "        if not chat_completion.choices or len(chat_completion.choices) == 0:\n",
    "            print(\"Error during hallucination detection: No response choices\")\n",
    "            return {\"soft_labels\": []}\n",
    "\n",
    "        raw_labels = chat_completion.choices[0].message.content\n",
    "\n",
    "        return extract_hallucination_positions(model_output_text, raw_labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "        return {\"soft_labels\": []}\n"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aebb245d-9f0d-4514-8e56-9c04a2d7f7ca"
   },
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0869c823-a9ae-4584-9b8e-e9bd1a2abae9",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.426030Z",
     "start_time": "2025-01-17T10:34:47.423905Z"
    }
   },
   "source": [
    "def hallucination_detect(question, model_output_text, context):\n",
    "    claims = extract_triplets_to_claims(question, model_output_text)\n",
    "    references = extract_and_get_references(claims, context)\n",
    "    hallucination_results = triplets_and_references_checker(claims, model_output_text, references, question)\n",
    "\n",
    "    soft_labels = hallucination_results.get(\"soft_labels\", [])\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    return soft_labels, hard_labels"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229008eb-a8d9-4738-ad74-a470376df9c2"
   },
   "source": [
    "## Apply on My Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.435051Z",
     "start_time": "2025-01-17T10:34:47.430492Z"
    }
   },
   "source": [
    "# process the dataset and save the results\n",
    "def process_dataset(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    input_files = glob.glob(os.path.join(input_folder, \"*.jsonl\"))\n",
    "\n",
    "    with tqdm(total=len(input_files), desc=\"Processing Files\", unit=\"file\") as file_progress:\n",
    "        for file_path in input_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "\n",
    "            output_data = []\n",
    "\n",
    "            with tqdm(total=len(data), desc=f\"Processing {os.path.basename(file_path)}\", unit=\"entry\",\n",
    "                      leave=False) as entry_progress:\n",
    "                for entry in data:\n",
    "                    try:\n",
    "                        question = entry.get(\"model_input\", \"\")\n",
    "                        model_output_text = entry.get(\"model_output_text\", \"\")\n",
    "                        context = entry.get(\"context_googlecse\", \"\")\n",
    "\n",
    "                        soft_labels, hard_labels = hallucination_detect(\n",
    "                            question, model_output_text, context\n",
    "                        )\n",
    "\n",
    "                        output_entry = {\n",
    "                            \"id\": entry.get(\"id\"),\n",
    "                            \"lang\": entry.get(\"lang\"),\n",
    "                            \"model_input\": entry.get(\"model_input\"),\n",
    "                            \"model_output_text\": entry.get(\"model_output_text\"),\n",
    "                            \"model_id\": entry.get(\"model_id\"),\n",
    "                            \"soft_labels\": soft_labels,\n",
    "                            \"hard_labels\": hard_labels,\n",
    "                            \"model_output_logits\": entry.get(\"model_output_logits\"),\n",
    "                            \"model_output_tokens\": entry.get(\"model_output_tokens\")\n",
    "                        }\n",
    "\n",
    "                        output_data.append(output_entry)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"OpenAI API Error: {e}\")\n",
    "                        # return []\n",
    "                        continue\n",
    "                    entry_progress.update(1)\n",
    "\n",
    "            output_file = os.path.join(output_folder, os.path.basename(file_path))\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for item in output_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            file_progress.update(1)\n",
    "            print(f\"Processed and saved: {output_file}\")"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "id": "83272c42-9948-4e8a-811c-b6cec8ec2dd0",
    "ExecuteTime": {
     "end_time": "2025-01-17T10:34:47.442252Z",
     "start_time": "2025-01-17T10:34:47.439677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_hallucination_positions(model_output_text, hallucination_results):\n",
    "    json_matches = re.findall(r'\\[\\s*\\{.*?\\}\\s*\\]', hallucination_results, re.DOTALL)\n",
    "\n",
    "    if not json_matches:\n",
    "        print(\"No valid JSON found. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    try:\n",
    "        hallucination_results = json.loads(json_matches[0])\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to decode extracted JSON. Returning empty labels.\")\n",
    "        return {\"soft_labels\": []}\n",
    "\n",
    "    soft_labels = []\n",
    "\n",
    "    # find the position in the original text\n",
    "    for result in hallucination_results:\n",
    "        word = result['word']\n",
    "        prob = result['prob']\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = model_output_text.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "\n",
    "            # save soft_labels\n",
    "            soft_labels.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"prob\": prob\n",
    "            })\n",
    "            start = end\n",
    "\n",
    "    return {\"soft_labels\": soft_labels}"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1626cda1-7e3b-4587-a048-98588f7be617",
    "outputId": "8daf4e01-05e8-41b5-8a16-3dc46c9939f5",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T12:37:58.924482Z",
     "start_time": "2025-01-17T10:34:47.447373Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def get_project_root():\n",
    "    return os.path.dirname(os.getcwd())\n",
    "\n",
    "input_folder = os.path.join(get_project_root(), \"data/exknowledge/\")\n",
    "output_folder = os.path.join(get_project_root(), \"data/detect_gpt/\")\n",
    "\n",
    "print(\"Input Folder Absolute Path:\", input_folder)\n",
    "process_dataset(input_folder, output_folder)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Folder Absolute Path: /Users/wt/SemEvalTask3/NCL-UoR/Jalynn/Method1/data/exknowledge/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/10 [00:00<?, ?file/s]\n",
      "Processing mushroom.ar-val.v2.jsonl:   0%|          | 0/50 [00:00<?, ?entry/s]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:   2%|▏         | 1/50 [01:15<1:01:27, 75.26s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:   4%|▍         | 2/50 [01:51<41:46, 52.21s/entry]  \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:   6%|▌         | 3/50 [06:54<2:10:34, 166.68s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:   8%|▊         | 4/50 [07:55<1:35:53, 125.07s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  10%|█         | 5/50 [08:36<1:11:01, 94.71s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  12%|█▏        | 6/50 [13:24<1:57:45, 160.58s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  14%|█▍        | 7/50 [15:25<1:45:41, 147.47s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  16%|█▌        | 8/50 [16:05<1:19:17, 113.26s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  18%|█▊        | 9/50 [18:35<1:25:11, 124.67s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  20%|██        | 10/50 [19:41<1:11:04, 106.61s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  22%|██▏       | 11/50 [22:51<1:25:52, 132.11s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  24%|██▍       | 12/50 [25:05<1:24:06, 132.81s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  26%|██▌       | 13/50 [25:47<1:04:51, 105.17s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  28%|██▊       | 14/50 [26:38<53:16, 88.78s/entry]   \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  30%|███       | 15/50 [27:43<47:43, 81.80s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  32%|███▏      | 16/50 [28:37<41:36, 73.41s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  34%|███▍      | 17/50 [29:23<35:46, 65.05s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  36%|███▌      | 18/50 [30:20<33:23, 62.61s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  38%|███▊      | 19/50 [32:57<47:00, 90.99s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  40%|████      | 20/50 [34:45<48:02, 96.09s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  42%|████▏     | 21/50 [35:14<36:46, 76.07s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  44%|████▍     | 22/50 [36:15<33:22, 71.50s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  46%|████▌     | 23/50 [37:49<35:16, 78.39s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  48%|████▊     | 24/50 [39:42<38:26, 88.71s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  50%|█████     | 25/50 [40:57<35:16, 84.65s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  52%|█████▏    | 26/50 [42:54<37:40, 94.18s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  54%|█████▍    | 27/50 [43:55<32:20, 84.35s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  56%|█████▌    | 28/50 [48:17<50:29, 137.70s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  58%|█████▊    | 29/50 [49:00<38:15, 109.32s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  60%|██████    | 30/50 [49:39<29:18, 87.94s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  62%|██████▏   | 31/50 [51:09<28:06, 88.74s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  64%|██████▍   | 32/50 [53:41<32:17, 107.66s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  66%|██████▌   | 33/50 [55:56<32:51, 115.99s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  68%|██████▊   | 34/50 [57:02<26:53, 100.84s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  70%|███████   | 35/50 [58:02<22:09, 88.61s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  72%|███████▏  | 36/50 [1:00:48<26:07, 111.94s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  74%|███████▍  | 37/50 [1:02:09<22:12, 102.48s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  76%|███████▌  | 38/50 [1:02:51<16:53, 84.45s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  78%|███████▊  | 39/50 [1:04:00<14:36, 79.70s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  80%|████████  | 40/50 [1:04:50<11:48, 70.81s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  82%|████████▏ | 41/50 [1:05:31<09:17, 61.92s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  84%|████████▍ | 42/50 [1:06:25<07:57, 59.69s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  86%|████████▌ | 43/50 [1:10:26<13:17, 113.98s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  88%|████████▊ | 44/50 [1:11:26<09:46, 97.81s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  90%|█████████ | 45/50 [1:13:20<08:32, 102.50s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  92%|█████████▏| 46/50 [1:13:52<05:26, 81.56s/entry] \u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  94%|█████████▍| 47/50 [1:14:37<03:31, 70.40s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  96%|█████████▌| 48/50 [1:16:26<02:44, 82.22s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl:  98%|█████████▊| 49/50 [1:17:06<01:09, 69.36s/entry]\u001B[A\n",
      "Processing mushroom.ar-val.v2.jsonl: 100%|██████████| 50/50 [1:17:58<00:00, 64.28s/entry]\u001B[A\n",
      "Processing Files:  10%|█         | 1/10 [1:17:58<11:41:49, 4678.78s/file]                \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /Users/wt/SemEvalTask3/NCL-UoR/Jalynn/Method1/data/detect_gpt/mushroom.ar-val.v2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.es-val.v2.jsonl:   0%|          | 0/50 [00:00<?, ?entry/s]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:   2%|▏         | 1/50 [03:31<2:52:38, 211.40s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:   4%|▍         | 2/50 [04:33<1:38:39, 123.31s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:   6%|▌         | 3/50 [05:40<1:16:42, 97.92s/entry] \u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:   8%|▊         | 4/50 [16:23<3:59:50, 312.84s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 24280 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.es-val.v2.jsonl:  10%|█         | 5/50 [18:29<3:04:16, 245.71s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  12%|█▏        | 6/50 [20:32<2:29:25, 203.77s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  14%|█▍        | 7/50 [31:36<4:13:56, 354.35s/entry]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 22008 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mushroom.es-val.v2.jsonl:  16%|█▌        | 8/50 [35:02<3:35:00, 307.15s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  18%|█▊        | 9/50 [36:59<2:49:20, 247.82s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  20%|██        | 10/50 [41:09<2:45:32, 248.32s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  22%|██▏       | 11/50 [42:17<2:05:39, 193.32s/entry]\u001B[A\n",
      "Processing mushroom.es-val.v2.jsonl:  24%|██▍       | 12/50 [45:03<1:57:04, 184.86s/entry]\u001B[A\n",
      "Processing Files:  10%|█         | 1/10 [2:03:11<18:28:41, 7391.31s/file]                 \u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m output_folder \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_project_root(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/detect_gpt/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput Folder Absolute Path:\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_folder)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_folder\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[110], line 21\u001B[0m, in \u001B[0;36mprocess_dataset\u001B[0;34m(input_folder, output_folder)\u001B[0m\n\u001B[1;32m     18\u001B[0m model_output_text \u001B[38;5;241m=\u001B[39m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_text\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m context \u001B[38;5;241m=\u001B[39m entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_googlecse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m soft_labels, hard_labels \u001B[38;5;241m=\u001B[39m \u001B[43mhallucination_detect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_output_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m output_entry \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: entry\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     35\u001B[0m }\n\u001B[1;32m     37\u001B[0m output_data\u001B[38;5;241m.\u001B[39mappend(output_entry)\n",
      "Cell \u001B[0;32mIn[109], line 3\u001B[0m, in \u001B[0;36mhallucination_detect\u001B[0;34m(question, model_output_text, context)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhallucination_detect\u001B[39m(question, model_output_text, context):\n\u001B[1;32m      2\u001B[0m     claims \u001B[38;5;241m=\u001B[39m extract_triplets_to_claims(question, model_output_text)\n\u001B[0;32m----> 3\u001B[0m     references \u001B[38;5;241m=\u001B[39m \u001B[43mextract_and_get_references\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclaims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     hallucination_results \u001B[38;5;241m=\u001B[39m triplets_and_references_checker(claims, model_output_text, references, question)\n\u001B[1;32m      6\u001B[0m     soft_labels \u001B[38;5;241m=\u001B[39m hallucination_results\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoft_labels\u001B[39m\u001B[38;5;124m\"\u001B[39m, [])\n",
      "Cell \u001B[0;32mIn[106], line 4\u001B[0m, in \u001B[0;36mextract_and_get_references\u001B[0;34m(claims, context)\u001B[0m\n\u001B[1;32m      2\u001B[0m references \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m claim \u001B[38;5;129;01min\u001B[39;00m claims:\n\u001B[0;32m----> 4\u001B[0m     verified_reference \u001B[38;5;241m=\u001B[39m \u001B[43mget_reference_for_claim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclaim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     references\u001B[38;5;241m.\u001B[39mappend(verified_reference)\n\u001B[1;32m      7\u001B[0m final_reference \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(references) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m context\n",
      "Cell \u001B[0;32mIn[105], line 11\u001B[0m, in \u001B[0;36mget_reference_for_claim\u001B[0;34m(claim)\u001B[0m\n\u001B[1;32m      2\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mPlease expand, provide additional relevant factual information and verify the following claim:\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124mClaims: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclaim\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124mIf the claim is inaccurate or incomplete, return a corrected, more detailed statement.\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 11\u001B[0m     chat_completion \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou are an AI assistant verifying claims.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m chat_completion\u001B[38;5;241m.\u001B[39mchoices \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(chat_completion\u001B[38;5;241m.\u001B[39mchoices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo response for the prompt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    856\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    857\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m    858\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m--> 859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    882\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    896\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:1283\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1271\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1279\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1280\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1281\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1282\u001B[0m     )\n\u001B[0;32m-> 1283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:960\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    958\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 960\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/openai/_base_client.py:996\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m    993\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSending HTTP Request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mmethod, request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    995\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 996\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    997\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    998\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    999\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1000\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1002\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36msend\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36m_send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    939\u001B[0m         response\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    940\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m--> 942\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_send_handling_auth\u001B[39m(\n\u001B[1;32m    943\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    944\u001B[0m     request: Request,\n\u001B[1;32m    945\u001B[0m     auth: Auth,\n\u001B[1;32m    946\u001B[0m     follow_redirects: \u001B[38;5;28mbool\u001B[39m,\n\u001B[1;32m    947\u001B[0m     history: \u001B[38;5;28mlist\u001B[39m[Response],\n\u001B[1;32m    948\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[1;32m    949\u001B[0m     auth_flow \u001B[38;5;241m=\u001B[39m auth\u001B[38;5;241m.\u001B[39msync_auth_flow(request)\n\u001B[1;32m    950\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36m_send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    974\u001B[0m         auth_flow\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_send_handling_redirects\u001B[39m(\n\u001B[1;32m    977\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    978\u001B[0m     request: Request,\n\u001B[0;32m--> 979\u001B[0m     follow_redirects: \u001B[38;5;28mbool\u001B[39m,\n\u001B[1;32m    980\u001B[0m     history: \u001B[38;5;28mlist\u001B[39m[Response],\n\u001B[1;32m    981\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[1;32m    982\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    983\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(history) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_redirects:\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001B[0m, in \u001B[0;36m_send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_send_single_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, request: Request) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[0;32m-> 1014\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;124;03m    Sends a single request, without handling any redirections.\u001B[39;00m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m     transport \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transport_for_url(request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m   1018\u001B[0m     timer \u001B[38;5;241m=\u001B[39m Timer()\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001B[0m, in \u001B[0;36mhandle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    253\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    232\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[1;32m    244\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[1;32m     99\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    100\u001B[0m     (\n\u001B[1;32m    101\u001B[0m         http_version,\n\u001B[1;32m    102\u001B[0m         status,\n\u001B[1;32m    103\u001B[0m         reason_phrase,\n\u001B[1;32m    104\u001B[0m         headers,\n\u001B[1;32m    105\u001B[0m         trailing_data,\n\u001B[0;32m--> 106\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    108\u001B[0m         http_version,\n\u001B[1;32m    109\u001B[0m         status,\n\u001B[1;32m    110\u001B[0m         reason_phrase,\n\u001B[1;32m    111\u001B[0m         headers,\n\u001B[1;32m    112\u001B[0m     )\n\u001B[1;32m    114\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    174\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 177\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[1;32m    179\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    214\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 217\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/SemEvalTask3/NCL-UoR/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1263\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[0;34m(self, buflen, flags)\u001B[0m\n\u001B[1;32m   1259\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1260\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1261\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1262\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1264\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1136\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1136\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1137\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[1;32m   1138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j19zkiJaJZPJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ade0f719-521a-462b-8812-19156817aba5",
    "ExecuteTime": {
     "end_time": "2025-01-17T12:37:58.934907Z",
     "start_time": "2025-01-17T07:16:34.129246Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from scorer import load_jsonl_file_to_records, score_iou, score_cor, main, recompute_hard_labels\n",
    "import argparse as ap\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7e0884a4-b040-4a29-a25c-9e209a2d10d5",
    "outputId": "03d4bbbb-6a4f-4958-f361-d71c127b8838",
    "ExecuteTime": {
     "end_time": "2025-01-17T12:37:58.936279Z",
     "start_time": "2025-01-17T07:16:34.171122Z"
    }
   },
   "source": [
    "def evaluate_iou_and_cor(val_dir, detect_dir, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\n",
    "\n",
    "    :param val_dir: Directory containing the ground truth files (e.g., data/val/val/)\n",
    "    :param detect_dir: Directory containing the detected files (e.g., data/detect/)\n",
    "    :param output_file: Path to save the evaluation results (optional)\n",
    "    \"\"\"\n",
    "    # List all files in the validation directory\n",
    "    val_files = os.listdir(val_dir)\n",
    "    detect_files = os.listdir(detect_dir)\n",
    "\n",
    "    # Ensure that we are comparing the same files (same lang)\n",
    "    for val_file in val_files:\n",
    "        # Skip non-JSONL files\n",
    "        if not val_file.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        # Check if the corresponding detect file exists\n",
    "        detect_file_path = os.path.join(detect_dir, val_file)\n",
    "\n",
    "        if not os.path.exists(detect_file_path):\n",
    "            print(f\"Warning: {detect_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load ground truth (val) and detected (detect) data\n",
    "        ref_dicts = load_jsonl_file_to_records(os.path.join(val_dir, val_file))\n",
    "        pred_dicts = load_jsonl_file_to_records(detect_file_path)\n",
    "\n",
    "        # Calculate IoU and Spearman correlation\n",
    "        try:\n",
    "            ious, cors = main(ref_dicts, pred_dicts)\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError occurred for file: {val_file}, skipping this file. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Print or save the results\n",
    "        print(f\"Results for {val_file}:\")\n",
    "        print(f\"  Mean IoU: {ious.mean():.8f}\")\n",
    "        print(f\"  Mean Spearman Correlation: {cors.mean():.8f}\")\n",
    "\n",
    "        # Optionally, save the results to a file\n",
    "        if output_file:\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"Results for {val_file}:\\n\")\n",
    "                f.write(f\"  Mean IoU: {ious.mean():.8f}\\n\")\n",
    "                f.write(f\"  Mean Spearman Correlation: {cors.mean():.8f}\\n\\n\")\n",
    "\n",
    "\n",
    "val_dir = 'data/val/val/'\n",
    "detect_dir = 'data/detect_gpt/'\n",
    "output_file = 'evaluation_results_gpt.txt'\n",
    "evaluate_iou_and_cor(val_dir, detect_dir, output_file)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/val/val/'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m detect_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/val/detect_2/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     52\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevaluation_results3.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 53\u001B[0m \u001B[43mevaluate_iou_and_cor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetect_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[65], line 10\u001B[0m, in \u001B[0;36mevaluate_iou_and_cor\u001B[0;34m(val_dir, detect_dir, output_file)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mEvaluate IoU and Spearman correlation between the reference (val) and detected (detect) files.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m:param output_file: Path to save the evaluation results (optional)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# List all files in the validation directory\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m val_files \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m detect_files \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(detect_dir)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Ensure that we are comparing the same files (same lang)\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/val/val/'"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTMSFVvdGQd2R8L2Q0cnpL",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
